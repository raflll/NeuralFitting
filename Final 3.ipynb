{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "129a6528",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656e8eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp39-cp39-win_amd64.whl (375.7 MB)\n",
      "     -------------------------------------- 375.7/375.7 MB 6.8 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 92.3 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "     ---------------------------------------- 135.6/135.6 KB ? eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\owner\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\owner\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (4.13.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-5.29.4-cp39-cp39-win_amd64.whl (434 kB)\n",
      "     ---------------------------------------- 434.6/434.6 KB ? eta 0:00:00\n",
      "Collecting h5py>=3.11.0\n",
      "  Downloading h5py-3.13.0-cp39-cp39-win_amd64.whl (3.0 MB)\n",
      "     ---------------------------------------- 3.0/3.0 MB 95.2 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.71.0-cp39-cp39-win_amd64.whl (4.3 MB)\n",
      "     ---------------------------------------- 4.3/4.3 MB 90.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\owner\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (25.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-win_amd64.whl (38 kB)\n",
      "Collecting numpy<2.2.0,>=1.26.0\n",
      "  Downloading numpy-2.0.2-cp39-cp39-win_amd64.whl (15.9 MB)\n",
      "     --------------------------------------- 15.9/15.9 MB 93.9 MB/s eta 0:00:00\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1\n",
      "  Downloading ml_dtypes-0.5.1-cp39-cp39-win_amd64.whl (209 kB)\n",
      "     ---------------------------------------- 209.4/209.4 KB ? eta 0:00:00\n",
      "Collecting tensorboard~=2.19.0\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 116.9 MB/s eta 0:00:00\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\owner\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tensorflow) (58.1.0)\n",
      "Collecting keras>=3.5.0\n",
      "  Downloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 88.8 MB/s eta 0:00:00\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Collecting optree\n",
      "  Downloading optree-0.15.0-cp39-cp39-win_amd64.whl (292 kB)\n",
      "     ---------------------------------------- 292.3/292.3 KB ? eta 0:00:00\n",
      "Collecting namex\n",
      "  Downloading namex-0.0.9-py3-none-any.whl (5.8 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "     ---------------------------------------- 243.2/243.2 KB ? eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-cp39-cp39-win_amd64.whl (105 kB)\n",
      "     ---------------------------------------- 105.8/105.8 KB ? eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "     -------------------------------------- 159.6/159.6 KB 9.3 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "     ---------------------------------------- 128.7/128.7 KB ? eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "     ---------------------------------------- 106.2/106.2 KB ? eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\owner\\appdata\\roaming\\python\\python39\\site-packages (from markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (8.7.0)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-3.0.2-cp39-cp39-win_amd64.whl (15 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\owner\\appdata\\roaming\\python\\python39\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\owner\\appdata\\roaming\\python\\python39\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (3.21.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, MarkupSafe, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, absl-py, werkzeug, requests, ml-dtypes, markdown-it-py, markdown, h5py, astunparse, tensorboard, rich, keras, tensorflow\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.2.2 astunparse-1.6.3 certifi-2025.4.26 charset-normalizer-3.4.2 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 idna-3.10 keras-3.9.2 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.9 numpy-2.0.2 opt-einsum-3.4.0 optree-0.15.0 protobuf-5.29.4 requests-2.32.3 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-3.1.0 urllib3-2.4.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script wheel.exe is installed in 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown-it.exe is installed in 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown_py.exe is installed in 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe and toco.exe are installed in 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f334b81b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './neural_responses_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# load neural responses. It is a np array with shape (200, 5) which corresponds\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# to 200 images and 5 neurons.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m neural_response_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./neural_responses_train.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Code below is for loading and preprocessing images.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Images will be loaded in train_iterator and test_iterator, which are two lists.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# elements in the list are np arrays with shape (10,224,224,3). 10 is batch size.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Images are 224x224 with 3 color channels.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m train_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./images/train\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\_npyio_impl.py:455\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    453\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 455\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    456\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './neural_responses_train.npy'"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# load neural responses. It is a np array with shape (200, 5) which corresponds\n",
    "# to 200 images and 5 neurons.\n",
    "neural_response_train = np.load(\"./neural_responses_train.npy\")\n",
    "\n",
    "\n",
    "# Code below is for loading and preprocessing images.\n",
    "# Images will be loaded in train_iterator and test_iterator, which are two lists.\n",
    "# elements in the list are np arrays with shape (10,224,224,3). 10 is batch size.\n",
    "# Images are 224x224 with 3 color channels.\n",
    "train_path = \"./images/train\"\n",
    "# test_path = \"./images/test\"\n",
    "\n",
    "batch_size = 10\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "trainfiles = [join(train_path, f) for f in listdir(train_path) if isfile(join(train_path, f))]\n",
    "trainfiles = sorted(trainfiles)\n",
    "\n",
    "# testfiles = [join(test_path, f) for f in listdir(test_path) if isfile(join(test_path, f))]\n",
    "# testfiles = sorted(testfiles)\n",
    "\n",
    "def load_image(im_file):\n",
    "  image = tf.io.decode_jpeg(tf.io.read_file(im_file), channels=3)\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = tf.image.resize(image, [img_height, img_width])\n",
    "  image = tf.keras.applications.vgg16.preprocess_input(image)\n",
    "  return image\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(trainfiles)\n",
    "train_dataset = train_dataset.map(load_image)\n",
    "train_dataset = train_dataset.batch(10)\n",
    "train_iterator = list(train_dataset.as_numpy_iterator())\n",
    "\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices(testfiles)\n",
    "# test_dataset = test_dataset.map(load_image)\n",
    "# test_dataset = test_dataset.batch(10)\n",
    "# test_iterator = list(test_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311079c",
   "metadata": {},
   "source": [
    "## Big Boi GridSearch (set up a shrine and pray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e275348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge, Lasso, SGDRegressor, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import pearsonr\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, DotProduct, WhiteKernel\n",
    "\n",
    "# Define a more robust Pearson correlation coefficient scorer\n",
    "def pearson_scorer(y_true, y_pred):\n",
    "    try:\n",
    "        # Check for constant values\n",
    "        if np.all(y_true == y_true[0]) or np.all(y_pred == y_pred[0]):\n",
    "            return 0.0  # Return 0 for constant predictions/targets\n",
    "            \n",
    "        # Check for NaN/Inf values\n",
    "        if np.any(np.isnan(y_true)) or np.any(np.isnan(y_pred)) or \\\n",
    "           np.any(np.isinf(y_true)) or np.any(np.isinf(y_pred)):\n",
    "            return 0.0  # Return 0 for NaN/Inf values\n",
    "            \n",
    "        # Calculate Pearson correlation\n",
    "        r, _ = pearsonr(y_true, y_pred)\n",
    "        \n",
    "        # Handle NaN or Inf in the result\n",
    "        if np.isnan(r) or np.isinf(r):\n",
    "            return 0.0\n",
    "            \n",
    "        return r\n",
    "    except Exception:\n",
    "        # Return 0 for any other error\n",
    "        return 0.0\n",
    "\n",
    "# Create the scorer\n",
    "pearson_scorer_sklearn = make_scorer(pearson_scorer, greater_is_better=True)\n",
    "\n",
    "def analyze_neuron(train_iterator, neural_response_train, neuron_idx=0, \n",
    "                  cnn_models=None, max_layers=3, regression_models=None):\n",
    "    \"\"\"\n",
    "    Find the best CNN, layer, and regression model for a specific neuron.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_iterator : Iterator\n",
    "        Iterator that yields batches of images\n",
    "    neural_response_train : numpy.ndarray\n",
    "        Array of neural responses with shape (n_samples, n_neurons)\n",
    "    neuron_idx : int, default=0\n",
    "        Index of the neuron to train the model for (0-based)\n",
    "    cnn_models : list or None, default=None\n",
    "        List of CNN models to test: 'vgg16', 'resnet', 'efficientnet', 'inception'\n",
    "    max_layers : int, default=3\n",
    "        Maximum number of layers to test per CNN architecture\n",
    "    regression_models : list or None, default=None\n",
    "        List of regression models to test\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the results for the best CNN, layer and model\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define CNN models to test\n",
    "    if cnn_models is None:\n",
    "        cnn_models = ['vgg16', 'resnet', 'efficientnet', 'inception']\n",
    "\n",
    "    # Define regression models to test (big boi list)\n",
    "    if regression_models is None:\n",
    "        regression_models = [\n",
    "            'ridge', 'elasticnet', 'lasso', 'bayesianridge', \n",
    "            'randomforest', 'sgd', 'huber',\n",
    "            'knn', 'adaboost', 'extratrees', 'gpr', 'linearsvr'\n",
    "        ]\n",
    "    \n",
    "    print(f\"Finding best model for Neuron {neuron_idx+1}\")\n",
    "    print(f\"CNN models to test: {cnn_models}\")\n",
    "    print(f\"Regression models to test: {regression_models}\")\n",
    "    \n",
    "    # Store results for each CNN architecture\n",
    "    all_cnn_results = []\n",
    "    \n",
    "    # Test each CNN architecture\n",
    "    for cnn_type in cnn_models:\n",
    "        best_layer_result = find_best_layer(\n",
    "            train_iterator=train_iterator,\n",
    "            neural_response_train=neural_response_train,\n",
    "            neuron_idx=neuron_idx,\n",
    "            model_type=cnn_type,\n",
    "            max_layers=max_layers,\n",
    "            regression_models=regression_models\n",
    "        )\n",
    "        if best_layer_result:\n",
    "            all_cnn_results.append(best_layer_result)\n",
    "    \n",
    "    # Compare results from all CNN architectures\n",
    "    if all_cnn_results:\n",
    "        print(\"\\nComparing best results from all CNN architectures:\")\n",
    "        print(f\"{'CNN Type':^12}|{'Best Layer':^22}|{'Regression Model':^18}|{'Test r':^10}|{'p-value':^10}\")\n",
    "        \n",
    "        for res in all_cnn_results:\n",
    "            print(f\"{res['cnn_type'].upper():^12}|{res['layer_name']:^22}|{res['regression_model_type']:^18}|{res['test_pearson_r']:.4f}|{res['test_pearson_p']:.4g}\")\n",
    "        \n",
    "        # Select the best overall CNN and layer\n",
    "        all_cnn_results.sort(key=lambda x: x['test_pearson_r'], reverse=True)\n",
    "        best_result = all_cnn_results[0]\n",
    "        \n",
    "        print(f\"\\n{best_result['cnn_type'].upper()} performs best for Neuron {neuron_idx+1}\")\n",
    "        print(f\"Best layer: {best_result['layer_name']}\")\n",
    "        print(f\"Best regression model: {best_result['regression_model_type']}\")\n",
    "        print(f\"Test Pearson's r: {best_result['test_pearson_r']:.4f} (p={best_result['test_pearson_p']:.4g})\")\n",
    "        \n",
    "        # Add neuron index to the results\n",
    "        best_result['neuron'] = neuron_idx + 1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "        print(f\"Runtime: {runtime:.2f} seconds ({runtime/60:.2f} minutes)\")\n",
    "        \n",
    "        return best_result\n",
    "    else:\n",
    "        print(\"No valid results found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_best_layer(train_iterator, neural_response_train, neuron_idx=0, \n",
    "                    model_type='vgg16', max_layers=3, regression_models=None):\n",
    "    \"\"\"\n",
    "    Find the best CNN layer for predicting a specific neuron's responses.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_iterator : Iterator\n",
    "        Iterator that yields batches of images\n",
    "    neural_response_train : numpy.ndarray\n",
    "        Array of neural responses with shape (n_samples, n_neurons)\n",
    "    neuron_idx : int, default=0\n",
    "        Index of the neuron to train the model for (0-based)\n",
    "    model_type : str, default='vgg16'\n",
    "        Type of CNN model to use ('vgg16', 'resnet', 'efficientnet', or 'inception')\n",
    "    max_layers : int, default=3\n",
    "        Maximum number of layers to test\n",
    "    regression_models : list or None, default=None\n",
    "        List of regression models to test\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict or None\n",
    "        Dictionary containing the results for the best layer and model, or None if no valid results\n",
    "    \"\"\"\n",
    "    # Define layers to test based on selected model\n",
    "    # Define layers to test based on selected model\n",
    "    if model_type.lower() == 'vgg16':\n",
    "        # All convolutional and pooling layers in VGG16\n",
    "        all_cnn_layers = [\n",
    "            # Block 1\n",
    "            'block1_conv1', 'block1_conv2', 'block1_pool',\n",
    "            # Block 2\n",
    "            'block2_conv1', 'block2_conv2', 'block2_pool',\n",
    "            # Block 3\n",
    "            'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_pool',\n",
    "            # Block 4\n",
    "            'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_pool',\n",
    "            # Block 5\n",
    "            'block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_pool'\n",
    "        ]\n",
    "        # Initialize VGG16 model\n",
    "        base_model = VGG16(weights='imagenet', include_top=False)\n",
    "        print(f\"Initialized VGG16 model for feature extraction\")\n",
    "        \n",
    "    elif model_type.lower() == 'resnet':\n",
    "        # Key layers in ResNet50\n",
    "        all_cnn_layers = [\n",
    "            # Stage 1\n",
    "            'conv1_relu', 'pool1_pool',\n",
    "            # Stage 2\n",
    "            'conv2_block1_out', 'conv2_block2_out', 'conv2_block3_out',\n",
    "            # Stage 3\n",
    "            'conv3_block1_out', 'conv3_block2_out', 'conv3_block3_out', 'conv3_block4_out',\n",
    "            # Stage 4\n",
    "            'conv4_block1_out', 'conv4_block2_out', 'conv4_block3_out', \n",
    "            'conv4_block4_out', 'conv4_block5_out', 'conv4_block6_out',\n",
    "            # Stage 5\n",
    "            'conv5_block1_out', 'conv5_block2_out', 'conv5_block3_out'\n",
    "        ]\n",
    "        # Initialize ResNet50 model\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "        print(f\"Initialized ResNet50 model for feature extraction\")\n",
    "        \n",
    "    elif model_type.lower() == 'efficientnet':\n",
    "        # Key layers in EfficientNetB0\n",
    "        all_cnn_layers = [\n",
    "            'block1a_project_bn', 'block2a_project_bn', 'block2b_project_bn',\n",
    "            'block3a_project_bn', 'block3b_project_bn', 'block4a_project_bn',\n",
    "            'block4b_project_bn', 'block4c_project_bn', 'block5a_project_bn',\n",
    "            'block5b_project_bn', 'block5c_project_bn', 'block6a_project_bn',\n",
    "            'block6b_project_bn', 'block6c_project_bn', 'block6d_project_bn',\n",
    "            'block7a_project_bn', 'top_activation'\n",
    "        ]\n",
    "        # Initialize EfficientNetB0 model\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False)\n",
    "        print(f\"Initialized EfficientNetB0 model for feature extraction\")\n",
    "    \n",
    "    elif model_type.lower() == 'inception':\n",
    "        # Key layers in InceptionV3\n",
    "        all_cnn_layers = [\n",
    "            'activation', 'activation_1', 'activation_2', 'activation_3',\n",
    "            'activation_4', 'max_pooling2d', 'activation_5', 'activation_6',\n",
    "            'activation_7', 'activation_8', 'max_pooling2d_1', 'mixed0',\n",
    "            'mixed1', 'mixed2', 'mixed3', 'mixed4', 'mixed5',\n",
    "            'mixed6', 'mixed7', 'mixed8', 'mixed9', 'mixed10'\n",
    "        ]\n",
    "        # Initialize InceptionV3 model\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "        print(f\"Initialized InceptionV3 model for feature extraction\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'vgg16', 'resnet', 'efficientnet', or 'inception'\")\n",
    "    \n",
    "    # Limit the number of layers to test if max_layers is less than total layers\n",
    "    if max_layers < len(all_cnn_layers):\n",
    "        selected_indices = np.linspace(0, len(all_cnn_layers) - 1, max_layers, dtype=int)\n",
    "        layers_to_test = [all_cnn_layers[i] for i in selected_indices]\n",
    "    else:\n",
    "        layers_to_test = all_cnn_layers\n",
    "    \n",
    "    print(f\"Testing layers: {layers_to_test}\")\n",
    "    \n",
    "    # Store results for each layer\n",
    "    all_layer_results = []\n",
    "    \n",
    "    # Test each layer\n",
    "    for layer_name in layers_to_test:\n",
    "        print(f\"Evaluating layer: {layer_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Create feature extraction model with the current layer\n",
    "            feature_model = tf.keras.Model(inputs=base_model.input, \n",
    "                                          outputs=base_model.get_layer(layer_name).output)\n",
    "            \n",
    "            # Extract features from all batches\n",
    "            all_features = []\n",
    "            for batch in train_iterator:\n",
    "                # Extract features\n",
    "                features = feature_model(batch, training=False)\n",
    "                # Global average pooling\n",
    "                pooled = tf.keras.layers.GlobalAveragePooling2D()(features)\n",
    "                all_features.append(pooled.numpy())\n",
    "            \n",
    "            features = np.concatenate(all_features, axis=0)\n",
    "            # print(f\"Features shape: {features.shape}\")\n",
    "            \n",
    "            # Feature diagnostics\n",
    "            # print(f\"Feature statistics - min: {np.min(features)}, max: {np.max(features)}\")\n",
    "            # print(f\"Any NaN in features: {np.any(np.isnan(features))}\")\n",
    "            # print(f\"Any Inf in features: {np.any(np.isinf(features))}\")\n",
    "            \n",
    "            # Split data into train and test sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                features, neural_response_train, \n",
    "                test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Get target for the selected neuron\n",
    "            y_train_neuron = y_train[:, neuron_idx]\n",
    "            y_test_neuron = y_test[:, neuron_idx]\n",
    "            \n",
    "            # Target diagnostics\n",
    "            # print(f\"Target statistics - min: {np.min(y_train_neuron)}, max: {np.max(y_train_neuron)}\")\n",
    "            # print(f\"Target variance: {np.var(y_train_neuron)}\")\n",
    "            # print(f\"Any NaN in target: {np.any(np.isnan(y_train_neuron))}\")\n",
    "            \n",
    "            # Define cross-validation strategy - using 5-fold\n",
    "            cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            \n",
    "            # Define optimized model configurations\n",
    "            model_configs = {\n",
    "                'ridge': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', Ridge(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [50, 100, 200, 'all'],\n",
    "                        'model__alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "                        'model__fit_intercept': [True, False],\n",
    "                        'model__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag']\n",
    "                    }\n",
    "                },\n",
    "                'lasso': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', Lasso(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [50, 100, 200, 'all'],\n",
    "                        'model__alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "                        'model__fit_intercept': [True, False],\n",
    "                        'model__selection': ['cyclic', 'random'],\n",
    "                        'model__max_iter': [1000, 3000, 5000]\n",
    "                    }\n",
    "                },\n",
    "                'elasticnet': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', ElasticNet(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [50, 100, 200, 'all'],\n",
    "                        'model__alpha': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "                        'model__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "                        'model__fit_intercept': [True, False],\n",
    "                        'model__selection': ['cyclic', 'random'],\n",
    "                        'model__max_iter': [1000, 3000, 5000]\n",
    "                    }\n",
    "                },\n",
    "                'bayesianridge': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', BayesianRidge())\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [50, 100, 200, 'all'],\n",
    "                        'model__alpha_1': [1e-7, 1e-6, 1e-5, 1e-4],\n",
    "                        'model__alpha_2': [1e-7, 1e-6, 1e-5, 1e-4],\n",
    "                        'model__lambda_1': [1e-7, 1e-6, 1e-5, 1e-4],\n",
    "                        'model__lambda_2': [1e-7, 1e-6, 1e-5, 1e-4],\n",
    "                        'model__fit_intercept': [True, False],\n",
    "                        'model__alpha_init': [None, 1.0, 10.0]\n",
    "                    }\n",
    "                },\n",
    "                'randomforest': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', RandomForestRegressor(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [50, 100, 200, 'all'],\n",
    "                        'model__n_estimators': [50, 100, 200],\n",
    "                        'model__max_depth': [3, 5, 8, 12, None],\n",
    "                        'model__min_samples_leaf': [1, 3, 5, 10],\n",
    "                        'model__min_samples_split': [2, 5, 10],\n",
    "                        'model__max_features': ['sqrt', 'log2']\n",
    "                    }\n",
    "                },\n",
    "                'extratrees': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', ExtraTreesRegressor(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [50, 100, 200, 'all'],\n",
    "                        'model__n_estimators': [50, 100, 200],\n",
    "                        'model__max_depth': [3, 5, 8, None],\n",
    "                        'model__min_samples_leaf': [1, 3, 5],\n",
    "                        'model__max_features': ['sqrt', 'log2']\n",
    "                    }\n",
    "                },\n",
    "                'adaboost': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', AdaBoostRegressor(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [50, 100, 200, 'all'],\n",
    "                        'model__n_estimators': [50, 100, 200],\n",
    "                        'model__learning_rate': [0.01, 0.1, 1.0],\n",
    "                        'model__loss': ['linear', 'square', 'exponential']\n",
    "                    }\n",
    "                },\n",
    "                'linearsvr': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', LinearSVR(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [50, 100, 200, 'all'],\n",
    "                        'model__C': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "                        'model__epsilon': [0.0, 0.01, 0.1, 0.2],\n",
    "                        'model__loss': ['epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "                        'model__max_iter': [1000, 2000, 5000]\n",
    "                    }\n",
    "                },\n",
    "                'sgd': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', SGDRegressor(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [50, 100, 200, 'all'],\n",
    "                        'model__loss': ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "                        'model__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                        'model__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "                        'model__learning_rate': ['constant', 'optimal', 'adaptive'],\n",
    "                        'model__eta0': [0.01, 0.1, 0.5]\n",
    "                    }\n",
    "                },\n",
    "                'huber': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', HuberRegressor())\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [50, 100, 200, 'all'],\n",
    "                        'model__epsilon': [1.1, 1.35, 1.5, 2.0],\n",
    "                        'model__alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "                        'model__fit_intercept': [True, False],\n",
    "                        'model__max_iter': [100, 500, 1000]\n",
    "                    }\n",
    "                },\n",
    "                'knn': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', KNeighborsRegressor())\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [50, 100, 200, 'all'],\n",
    "                        'model__n_neighbors': [3, 5, 7, 9, 15],\n",
    "                        'model__weights': ['uniform', 'distance'],\n",
    "                        'model__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                        'model__leaf_size': [10, 30, 50]\n",
    "                    }\n",
    "                },\n",
    "                'gpr': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', GaussianProcessRegressor(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [50, 100, 'all'],\n",
    "                        'model__kernel': [\n",
    "                            RBF(), \n",
    "                            RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0), \n",
    "                            DotProduct() + WhiteKernel(noise_level=1.0)\n",
    "                        ],\n",
    "                        'model__alpha': [1e-10, 1e-8, 1e-6],\n",
    "                        'model__n_restarts_optimizer': [0, 1, 3]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Filter model configs based on requested regression models\n",
    "            filtered_configs = {name: config for name, config in model_configs.items() \n",
    "                               if name in regression_models}\n",
    "            \n",
    "            # Run grid search for each selected model\n",
    "            grid_results = {}\n",
    "            \n",
    "            for model_name, config in filtered_configs.items():\n",
    "                print(f\"  Testing {model_name.capitalize()}...\")\n",
    "                \n",
    "                grid = GridSearchCV(\n",
    "                    config['pipeline'],\n",
    "                    config['param_grid'],\n",
    "                    cv=cv,\n",
    "                    scoring=pearson_scorer_sklearn,  # Use Pearson's r\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                try:\n",
    "                    grid.fit(X_train, y_train_neuron)\n",
    "                    \n",
    "                    grid_results[model_name] = {\n",
    "                        'grid': grid,\n",
    "                        'best_score': grid.best_score_,\n",
    "                        'best_params': grid.best_params_\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  {model_name.capitalize()} CV Pearson's r: {grid.best_score_:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error training {model_name.capitalize()}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Find the best model overall\n",
    "            if grid_results:\n",
    "                best_model_name = max(grid_results, key=lambda k: grid_results[k]['best_score'])\n",
    "                best_model = grid_results[best_model_name]['grid'].best_estimator_\n",
    "                regression_model_type = best_model_name.capitalize()\n",
    "                best_params = grid_results[best_model_name]['best_params']\n",
    "                best_cv_score = grid_results[best_model_name]['best_score']\n",
    "                \n",
    "                # Evaluate on test set with the best model\n",
    "                y_train_pred = best_model.predict(X_train)\n",
    "                train_r, train_p = pearsonr(y_train_neuron, y_train_pred)\n",
    "                train_rmse = np.sqrt(mean_squared_error(y_train_neuron, y_train_pred))\n",
    "                \n",
    "                y_test_pred = best_model.predict(X_test)\n",
    "                test_r, test_p = pearsonr(y_test_neuron, y_test_pred)\n",
    "                test_rmse = np.sqrt(mean_squared_error(y_test_neuron, y_test_pred))\n",
    "                \n",
    "                # Print diagnostic info about predictions\n",
    "                print(f\"  Train predictions - min: {np.min(y_train_pred)}, max: {np.max(y_train_pred)}\")\n",
    "                print(f\"  Test predictions - min: {np.min(y_test_pred)}, max: {np.max(y_test_pred)}\")\n",
    "                print(f\"  Any NaN in predictions: {np.any(np.isnan(y_train_pred)) or np.any(np.isnan(y_test_pred))}\")\n",
    "                \n",
    "                print(f\"  Best model: {regression_model_type}\")\n",
    "                print(f\"  Best scaler: {best_model.named_steps['scaler'].__class__.__name__}\")\n",
    "                print(f\"  Train: r = {train_r:.4f} (p={train_p:.4g}), RMSE = {train_rmse:.4f}\")\n",
    "                print(f\"  Test: r = {test_r:.4f} (p={test_p:.4g}), RMSE = {test_rmse:.4f}\")\n",
    "                \n",
    "                # Store results for this layer\n",
    "                layer_results = {\n",
    "                    'cnn_type': model_type,\n",
    "                    'layer_name': layer_name,\n",
    "                    'regression_model_type': regression_model_type,\n",
    "                    'best_params': best_params,\n",
    "                    'scaler_type': best_model.named_steps['scaler'].__class__.__name__,\n",
    "                    'cv_pearson_r': best_cv_score,\n",
    "                    'train_pearson_r': train_r,\n",
    "                    'train_pearson_p': train_p,\n",
    "                    'test_pearson_r': test_r,\n",
    "                    'test_pearson_p': test_p,\n",
    "                    'train_rmse': train_rmse,\n",
    "                    'test_rmse': test_rmse,\n",
    "                    'model': best_model\n",
    "                }\n",
    "                \n",
    "                all_layer_results.append(layer_results)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing layer {layer_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort results based on test Pearson's r\n",
    "    if all_layer_results:\n",
    "        all_layer_results.sort(key=lambda x: x['test_pearson_r'], reverse=True)\n",
    "        \n",
    "        # Print summary of all layers\n",
    "        print(\"\\nSummary of Results:\")\n",
    "        print(f\"{'Layer Name':^22}|{'Regression Model':^18}|{'CV r':^10}|{'Test r':^10}\")\n",
    "        \n",
    "        for res in all_layer_results:\n",
    "            print(f\"{res['layer_name']:^22}|{res['regression_model_type']:^18}|{res['cv_pearson_r']:.4f}|{res['test_pearson_r']:.4f}\")\n",
    "        \n",
    "        # Return the best layer result\n",
    "        best_layer_result = all_layer_results[0]\n",
    "        return best_layer_result\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb9ac0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best model for Neuron 1\n",
      "CNN models to test: ['vgg16']\n",
      "Regression models to test: ['svr', 'sgd', 'huber', 'knn', 'gpr', 'linearsvr']\n",
      "Initialized VGG16 model for feature extraction\n",
      "Testing layers: ['block1_conv1']\n",
      "Evaluating layer: block1_conv1\n",
      "  Testing Linearsvr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ethan\\anaconda3\\envs\\nimbus\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ethan\\anaconda3\\envs\\nimbus\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Linearsvr CV Pearson's r: 0.0784\n",
      "  Testing Sgd...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ethan\\anaconda3\\envs\\nimbus\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:776: UserWarning: k=100 is greater than n_features=64. All the features will be returned.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ethan\\anaconda3\\envs\\nimbus\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1575: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sgd CV Pearson's r: 0.2502\n",
      "  Testing Huber...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ethan\\anaconda3\\envs\\nimbus\\Lib\\site-packages\\sklearn\\linear_model\\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Huber CV Pearson's r: 0.0581\n",
      "  Testing Knn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ethan\\anaconda3\\envs\\nimbus\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:776: UserWarning: k=100 is greater than n_features=64. All the features will be returned.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Knn CV Pearson's r: 0.0904\n",
      "  Testing Gpr...\n",
      "  Gpr CV Pearson's r: 0.0892\n",
      "  Train predictions - min: -5679.690546000719, max: 5435.543096577406\n",
      "  Test predictions - min: -5183.314569438219, max: 5732.039190327406\n",
      "  Any NaN in predictions: False\n",
      "  Best model: Sgd\n",
      "  Best scaler: RobustScaler\n",
      "  Train: r = 0.0138 (p=0.8628), RMSE = 1870.0378\n",
      "  Test: r = 0.1080 (p=0.5071), RMSE = 2091.7319\n",
      "\n",
      "Summary of Results:\n",
      "      Layer Name      | Regression Model |   CV r   |  Test r  \n",
      "     block1_conv1     |       Sgd        |0.2502|0.1080\n",
      "\n",
      "Comparing best results from all CNN architectures:\n",
      "  CNN Type  |      Best Layer      | Regression Model |  Test r  | p-value  \n",
      "   VGG16    |     block1_conv1     |       Sgd        |0.1080|0.5071\n",
      "\n",
      "VGG16 performs best for Neuron 1\n",
      "Best layer: block1_conv1\n",
      "Best regression model: Sgd\n",
      "Test Pearson's r: 0.1080 (p=0.5071)\n",
      "Runtime: 62.95 seconds (1.05 minutes)\n",
      "\n",
      "\n",
      "==========RESULT==========\n",
      "Best CNN: vgg16\n",
      "Best layer: block1_conv1\n",
      "Best scaler: RobustScaler\n",
      "Best regression model: Sgd\n",
      "Best params: {'feature_selection__k': 100, 'model__alpha': 0.1, 'model__eta0': 0.1, 'model__learning_rate': 'adaptive', 'model__loss': 'squared_epsilon_insensitive', 'model__penalty': 'l2', 'scaler': RobustScaler()}\n",
      "Train Pearson's r: 0.0138\n",
      "Test Pearson's r: 0.1080\n"
     ]
    }
   ],
   "source": [
    "result = analyze_neuron(\n",
    "    neuron_idx=0,\n",
    "    train_iterator=train_iterator,\n",
    "    neural_response_train=neural_response_train,\n",
    "    cnn_models=['vgg16', 'resnet', 'efficientnet', 'inception'],\n",
    "    regression_models=  ['ridge', 'elasticnet', 'lasso', 'bayesianridge', 'randomforest', 'sgd', 'huber', 'knn', 'adaboost', 'extratrees', 'gpr', 'linearsvr'],\n",
    "    max_layers=50\n",
    ")\n",
    "\n",
    "# Print quick summary\n",
    "print('\\n\\n'+'='*10+'RESULT'+'='*10)\n",
    "print(f\"Best CNN: {result['cnn_type']}\")\n",
    "print(f\"Best layer: {result['layer_name']}\")\n",
    "print(f\"Best scaler: {result['scaler_type']}\")\n",
    "print(f\"Best regression model: {result['regression_model_type']}\")\n",
    "print(f\"Best params: {result['best_params']}\")\n",
    "print(f\"Train Pearson's r: {result['train_pearson_r']:.4f}\")\n",
    "print(f\"Test Pearson's r: {result['test_pearson_r']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5cc51f",
   "metadata": {},
   "source": [
    "## Smaller boi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1482f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.linear_model import Ridge, ElasticNet, BayesianRidge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.stats import pearsonr\n",
    "import time\n",
    "\n",
    "# Define a more robust Pearson correlation coefficient scorer\n",
    "def pearson_scorer(y_true, y_pred):\n",
    "    try:\n",
    "        # Check for constant values\n",
    "        if np.all(y_true == y_true[0]) or np.all(y_pred == y_pred[0]):\n",
    "            return 0.0  # Return 0 for constant predictions/targets\n",
    "            \n",
    "        # Check for NaN/Inf values\n",
    "        if np.any(np.isnan(y_true)) or np.any(np.isnan(y_pred)) or \\\n",
    "           np.any(np.isinf(y_true)) or np.any(np.isinf(y_pred)):\n",
    "            return 0.0  # Return 0 for NaN/Inf values\n",
    "            \n",
    "        # Calculate Pearson correlation\n",
    "        r, _ = pearsonr(y_true, y_pred)\n",
    "        \n",
    "        # Handle NaN or Inf in the result\n",
    "        if np.isnan(r) or np.isinf(r):\n",
    "            return 0.0\n",
    "            \n",
    "        return r\n",
    "    except Exception:\n",
    "        # Return 0 for any other error\n",
    "        return 0.0\n",
    "\n",
    "# Create the scorer\n",
    "pearson_scorer_sklearn = make_scorer(pearson_scorer, greater_is_better=True)\n",
    "\n",
    "def analyze_neuron(train_iterator, neural_response_train, neuron_idx=0, \n",
    "                  cnn_models=None, max_layers=3, regression_models=None):\n",
    "    \"\"\"\n",
    "    Find the best CNN, layer, and regression model for a specific neuron.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_iterator : Iterator\n",
    "        Iterator that yields batches of images\n",
    "    neural_response_train : numpy.ndarray\n",
    "        Array of neural responses with shape (n_samples, n_neurons)\n",
    "    neuron_idx : int, default=0\n",
    "        Index of the neuron to train the model for (0-based)\n",
    "    cnn_models : list or None, default=None\n",
    "        List of CNN models to test: 'vgg16', 'resnet', 'efficientnet', 'inception'\n",
    "    max_layers : int, default=3\n",
    "        Maximum number of layers to test per CNN architecture\n",
    "    regression_models : list or None, default=None\n",
    "        List of regression models to test\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing the results for the best CNN, layer and model\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define CNN models to test\n",
    "    if cnn_models is None:\n",
    "        cnn_models = ['vgg16', 'resnet', 'efficientnet', 'inception']\n",
    "\n",
    "    # Define regression models to test (focused list for speed)\n",
    "    if regression_models is None:\n",
    "        regression_models = ['ridge', 'elasticnet', 'lasso', 'bayesianridge', 'randomforest', 'gbr', 'svr', 'linearsvr']\n",
    "    \n",
    "    print(f\"Finding best model for Neuron {neuron_idx+1}\")\n",
    "    print(f\"CNN models to test: {cnn_models}\")\n",
    "    print(f\"Regression models to test: {regression_models}\")\n",
    "    \n",
    "    # Store results for each CNN architecture\n",
    "    all_cnn_results = []\n",
    "    \n",
    "    # Test each CNN architecture\n",
    "    for cnn_type in cnn_models:\n",
    "        best_layer_result = find_best_layer(\n",
    "            train_iterator=train_iterator,\n",
    "            neural_response_train=neural_response_train,\n",
    "            neuron_idx=neuron_idx,\n",
    "            model_type=cnn_type,\n",
    "            max_layers=max_layers,\n",
    "            regression_models=regression_models\n",
    "        )\n",
    "        if best_layer_result:\n",
    "            all_cnn_results.append(best_layer_result)\n",
    "    \n",
    "    # Compare results from all CNN architectures\n",
    "    if all_cnn_results:\n",
    "        print(\"\\nComparing best results from all CNN architectures:\")\n",
    "        print(f\"{'CNN Type':^12}|{'Best Layer':^22}|{'Regression Model':^18}|{'Test r':^10}|{'p-value':^10}\")\n",
    "        \n",
    "        for res in all_cnn_results:\n",
    "            print(f\"{res['cnn_type'].upper():^12}|{res['layer_name']:^22}|{res['regression_model_type']:^18}|{res['test_pearson_r']:.4f}|{res['test_pearson_p']:.4g}\")\n",
    "        \n",
    "        # Select the best overall CNN and layer\n",
    "        all_cnn_results.sort(key=lambda x: x['test_pearson_r'], reverse=True)\n",
    "        best_result = all_cnn_results[0]\n",
    "        \n",
    "        print(f\"\\n{best_result['cnn_type'].upper()} performs best for Neuron {neuron_idx+1}\")\n",
    "        print(f\"Best layer: {best_result['layer_name']}\")\n",
    "        print(f\"Best regression model: {best_result['regression_model_type']}\")\n",
    "        print(f\"Test Pearson's r: {best_result['test_pearson_r']:.4f} (p={best_result['test_pearson_p']:.4g})\")\n",
    "        \n",
    "        # Add neuron index to the results\n",
    "        best_result['neuron'] = neuron_idx + 1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        runtime = end_time - start_time\n",
    "        print(f\"Runtime: {runtime:.2f} seconds ({runtime/60:.2f} minutes)\")\n",
    "        \n",
    "        return best_result\n",
    "    else:\n",
    "        print(\"No valid results found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_best_layer(train_iterator, neural_response_train, neuron_idx=0, \n",
    "                    model_type='vgg16', max_layers=3, regression_models=None):\n",
    "    \"\"\"\n",
    "    Find the best CNN layer for predicting a specific neuron's responses.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_iterator : Iterator\n",
    "        Iterator that yields batches of images\n",
    "    neural_response_train : numpy.ndarray\n",
    "        Array of neural responses with shape (n_samples, n_neurons)\n",
    "    neuron_idx : int, default=0\n",
    "        Index of the neuron to train the model for (0-based)\n",
    "    model_type : str, default='vgg16'\n",
    "        Type of CNN model to use ('vgg16', 'resnet', 'efficientnet', or 'inception')\n",
    "    max_layers : int, default=3\n",
    "        Maximum number of layers to test\n",
    "    regression_models : list or None, default=None\n",
    "        List of regression models to test\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict or None\n",
    "        Dictionary containing the results for the best layer and model, or None if no valid results\n",
    "    \"\"\"\n",
    "    # Define layers to test based on selected model\n",
    "    if model_type.lower() == 'vgg16':\n",
    "        all_cnn_layers = [\n",
    "            'block1_pool', 'block2_pool', 'block3_pool', 'block4_pool', 'block5_pool'\n",
    "        ]\n",
    "        base_model = VGG16(weights='imagenet', include_top=False)\n",
    "        print(f\"Testing VGG16 model\")\n",
    "        \n",
    "    elif model_type.lower() == 'resnet':\n",
    "        all_cnn_layers = [\n",
    "            'conv1_relu', 'conv2_block3_out', 'conv3_block4_out', \n",
    "            'conv4_block6_out', 'conv5_block3_out'\n",
    "        ]\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "        print(f\"Testing ResNet50 model\")\n",
    "        \n",
    "    elif model_type.lower() == 'efficientnet':\n",
    "        all_cnn_layers = [\n",
    "            'block2b_add', 'block3b_add', 'block4c_add',\n",
    "            'block5c_add', 'block6d_add', 'top_activation'\n",
    "        ]\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False)\n",
    "        print(f\"Testing EfficientNetB0 model\")\n",
    "    \n",
    "    elif model_type.lower() == 'inception':\n",
    "        all_cnn_layers = [\n",
    "            'max_pooling2d', 'max_pooling2d_1', 'mixed2', \n",
    "            'mixed5', 'mixed7', 'mixed10'\n",
    "        ]\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "        print(f\"Testing InceptionV3 model\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'vgg16', 'resnet', 'efficientnet', or 'inception'\")\n",
    "    \n",
    "    # Limit the number of layers to test if max_layers is less than total layers\n",
    "    if max_layers < len(all_cnn_layers):\n",
    "        selected_indices = np.linspace(0, len(all_cnn_layers) - 1, max_layers, dtype=int)\n",
    "        layers_to_test = [all_cnn_layers[i] for i in selected_indices]\n",
    "    else:\n",
    "        layers_to_test = all_cnn_layers\n",
    "    \n",
    "    print(f\"Testing layers: {layers_to_test}\")\n",
    "    \n",
    "    # Store results for each layer\n",
    "    all_layer_results = []\n",
    "    \n",
    "    # Test each layer\n",
    "    for layer_name in layers_to_test:\n",
    "        print(f\"Evaluating layer: {layer_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Create feature extraction model with the current layer\n",
    "            feature_model = tf.keras.Model(inputs=base_model.input, \n",
    "                                          outputs=base_model.get_layer(layer_name).output)\n",
    "            \n",
    "            # Extract features from all batches\n",
    "            all_features = []\n",
    "            for batch in train_iterator:\n",
    "                # Extract features\n",
    "                features = feature_model(batch, training=False)\n",
    "                # Global average pooling\n",
    "                pooled = tf.keras.layers.GlobalAveragePooling2D()(features)\n",
    "                all_features.append(pooled.numpy())\n",
    "            \n",
    "            features = np.concatenate(all_features, axis=0)\n",
    "            # print(f\"Features shape: {features.shape}\")\n",
    "            \n",
    "            # Feature diagnostics\n",
    "            # print(f\"Feature statistics - min: {np.min(features)}, max: {np.max(features)}\")\n",
    "            # print(f\"Any NaN in features: {np.any(np.isnan(features))}\")\n",
    "            # print(f\"Any Inf in features: {np.any(np.isinf(features))}\")\n",
    "            \n",
    "            # Split data into train and test sets\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                features, neural_response_train, \n",
    "                test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Get target for the selected neuron\n",
    "            y_train_neuron = y_train[:, neuron_idx]\n",
    "            y_test_neuron = y_test[:, neuron_idx]\n",
    "            \n",
    "            # Target diagnostics\n",
    "            # print(f\"Target statistics - min: {np.min(y_train_neuron)}, max: {np.max(y_train_neuron)}\")\n",
    "            # print(f\"Target variance: {np.var(y_train_neuron)}\")\n",
    "            # print(f\"Any NaN in target: {np.any(np.isnan(y_train_neuron))}\")\n",
    "            \n",
    "            # Define cross-validation strategy - using 5-fold\n",
    "            cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "            \n",
    "            # Define optimized model configurations\n",
    "            model_configs = {\n",
    "                'ridge': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', Ridge(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [100, 200, 'all'],\n",
    "                        'model__alpha': [0.1, 1.0, 10.0]\n",
    "                    }\n",
    "                },\n",
    "                'lasso': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', Lasso(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [100, 200, 'all'],\n",
    "                        'model__alpha': [0.01, 0.1, 1.0]\n",
    "                    }\n",
    "                },\n",
    "                'elasticnet': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', ElasticNet(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [100, 200, 'all'],\n",
    "                        'model__alpha': [0.01, 0.1, 1.0],\n",
    "                        'model__l1_ratio': [0.1, 0.5, 0.9]\n",
    "                    }\n",
    "                },\n",
    "                'bayesianridge': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', BayesianRidge())\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [100, 200, 'all'],\n",
    "                        'model__alpha_1': [1e-6, 1e-4],\n",
    "                        'model__alpha_2': [1e-6, 1e-4]\n",
    "                    }\n",
    "                },\n",
    "                'randomforest': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', RandomForestRegressor(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [100, 200, 'all'],\n",
    "                        'model__n_estimators': [50, 100],\n",
    "                        'model__max_depth': [5, 10]\n",
    "                    }\n",
    "                },\n",
    "                'gbr': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', GradientBoostingRegressor(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [100, 200, 'all'],\n",
    "                        'model__n_estimators': [50, 100],\n",
    "                        'model__learning_rate': [0.01, 0.1]\n",
    "                    }\n",
    "                },\n",
    "                'svr': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', SVR())\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [100, 200, 'all'],\n",
    "                        'model__C': [0.1, 1.0, 10.0],\n",
    "                        'model__kernel': ['linear', 'rbf']\n",
    "                    }\n",
    "                },\n",
    "                'linearsvr': {\n",
    "                    'pipeline': Pipeline([\n",
    "                        ('scaler', None),  # Will be set by grid search\n",
    "                        ('feature_selection', SelectKBest(f_regression)),\n",
    "                        ('model', LinearSVR(random_state=42))\n",
    "                    ]),\n",
    "                    'param_grid': {\n",
    "                        'scaler': [StandardScaler(), RobustScaler()],\n",
    "                        'feature_selection__k': [100, 200, 'all'],\n",
    "                        'model__C': [0.1, 1.0, 10.0]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Filter model configs based on requested regression models\n",
    "            filtered_configs = {name: config for name, config in model_configs.items() \n",
    "                               if name in regression_models}\n",
    "            \n",
    "            # Run grid search for each selected model\n",
    "            grid_results = {}\n",
    "            \n",
    "            for model_name, config in filtered_configs.items():\n",
    "                print(f\"  Testing {model_name.capitalize()}...\")\n",
    "                \n",
    "                grid = GridSearchCV(\n",
    "                    config['pipeline'],\n",
    "                    config['param_grid'],\n",
    "                    cv=cv,\n",
    "                    scoring=pearson_scorer_sklearn,  # Use Pearson's r\n",
    "                    n_jobs=-1,\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                try:\n",
    "                    grid.fit(X_train, y_train_neuron)\n",
    "                    \n",
    "                    grid_results[model_name] = {\n",
    "                        'grid': grid,\n",
    "                        'best_score': grid.best_score_,\n",
    "                        'best_params': grid.best_params_\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"  {model_name.capitalize()} CV Pearson's r: {grid.best_score_:.4f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error training {model_name.capitalize()}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Find the best model overall\n",
    "            if grid_results:\n",
    "                best_model_name = max(grid_results, key=lambda k: grid_results[k]['best_score'])\n",
    "                best_model = grid_results[best_model_name]['grid'].best_estimator_\n",
    "                regression_model_type = best_model_name.capitalize()\n",
    "                best_params = grid_results[best_model_name]['best_params']\n",
    "                best_cv_score = grid_results[best_model_name]['best_score']\n",
    "                \n",
    "                # Evaluate on test set with the best model\n",
    "                y_train_pred = best_model.predict(X_train)\n",
    "                train_r, train_p = pearsonr(y_train_neuron, y_train_pred)\n",
    "                train_rmse = np.sqrt(mean_squared_error(y_train_neuron, y_train_pred))\n",
    "                \n",
    "                y_test_pred = best_model.predict(X_test)\n",
    "                test_r, test_p = pearsonr(y_test_neuron, y_test_pred)\n",
    "                test_rmse = np.sqrt(mean_squared_error(y_test_neuron, y_test_pred))\n",
    "                \n",
    "                # Print diagnostic info about predictions\n",
    "                print(f\"  Train predictions - min: {np.min(y_train_pred)}, max: {np.max(y_train_pred)}\")\n",
    "                print(f\"  Test predictions - min: {np.min(y_test_pred)}, max: {np.max(y_test_pred)}\")\n",
    "                print(f\"  Any NaN in predictions: {np.any(np.isnan(y_train_pred)) or np.any(np.isnan(y_test_pred))}\")\n",
    "                \n",
    "                print(f\"  Best model: {regression_model_type}\")\n",
    "                print(f\"  Best scaler: {best_model.named_steps['scaler'].__class__.__name__}\")\n",
    "                print(f\"  Train: r = {train_r:.4f} (p={train_p:.4g}), RMSE = {train_rmse:.4f}\")\n",
    "                print(f\"  Test: r = {test_r:.4f} (p={test_p:.4g}), RMSE = {test_rmse:.4f}\")\n",
    "                \n",
    "                # Store results for this layer\n",
    "                layer_results = {\n",
    "                    'cnn_type': model_type,\n",
    "                    'layer_name': layer_name,\n",
    "                    'regression_model_type': regression_model_type,\n",
    "                    'best_params': best_params,\n",
    "                    'scaler_type': best_model.named_steps['scaler'].__class__.__name__,\n",
    "                    'cv_pearson_r': best_cv_score,\n",
    "                    'train_pearson_r': train_r,\n",
    "                    'train_pearson_p': train_p,\n",
    "                    'test_pearson_r': test_r,\n",
    "                    'test_pearson_p': test_p,\n",
    "                    'train_rmse': train_rmse,\n",
    "                    'test_rmse': test_rmse,\n",
    "                    'model': best_model\n",
    "                }\n",
    "                \n",
    "                all_layer_results.append(layer_results)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing layer {layer_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Sort results based on test Pearson's r\n",
    "    if all_layer_results:\n",
    "        all_layer_results.sort(key=lambda x: x['test_pearson_r'], reverse=True)\n",
    "        \n",
    "        # Print summary of all layers\n",
    "        print(\"\\nSummary of Results:\")\n",
    "        print(f\"{'Layer Name':^22}|{'Regression Model':^18}|{'CV r':^10}|{'Test r':^10}\")\n",
    "        \n",
    "        for res in all_layer_results:\n",
    "            print(f\"{res['layer_name']:^22}|{res['regression_model_type']:^18}|{res['cv_pearson_r']:.4f}|{res['test_pearson_r']:.4f}\")\n",
    "        \n",
    "        # Return the best layer result\n",
    "        best_layer_result = all_layer_results[0]\n",
    "        return best_layer_result\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6fb6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding best model for Neuron 1\n",
      "CNN models to test: ['vgg16']\n",
      "Regression models to test: ['ridge']\n",
      "Testing VGG16 model\n",
      "Testing layers: ['block1_pool', 'block5_pool']\n",
      "Evaluating layer: block1_pool\n",
      "Features shape: (200, 64)\n",
      "Feature statistics - min: 0.013451282866299152, max: 2302.58349609375\n",
      "Any NaN in features: False\n",
      "Any Inf in features: False\n",
      "Target statistics - min: 0.0, max: 3.267901659011841\n",
      "Target variance: 0.2589110732078552\n",
      "Any NaN in target: False\n",
      "  Testing Ridge...\n",
      "  Ridge CV Pearson's r: 0.0674\n",
      "  Train predictions - min: -0.23347827792167664, max: 0.8190208673477173\n",
      "  Test predictions - min: -0.18321236968040466, max: 0.9697402715682983\n",
      "  Any NaN in predictions: False\n",
      "  Best model: Ridge\n",
      "  Best scaler: StandardScaler\n",
      "  Train: r = 0.5929 (p=1.466e-16), RMSE = 0.4184\n",
      "  Test: r = 0.0184 (p=0.9105), RMSE = 0.8773\n",
      "Evaluating layer: block5_pool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ethan\\anaconda3\\envs\\nimbus\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:776: UserWarning: k=100 is greater than n_features=64. All the features will be returned.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (200, 512)\n",
      "Feature statistics - min: 0.0, max: 143.68621826171875\n",
      "Any NaN in features: False\n",
      "Any Inf in features: False\n",
      "Target statistics - min: 0.0, max: 3.267901659011841\n",
      "Target variance: 0.2589110732078552\n",
      "Any NaN in target: False\n",
      "  Testing Ridge...\n",
      "  Ridge CV Pearson's r: 0.3369\n",
      "  Train predictions - min: -0.014868184924125671, max: 3.2236123085021973\n",
      "  Test predictions - min: -0.4157315790653229, max: 2.239962100982666\n",
      "  Any NaN in predictions: False\n",
      "  Best model: Ridge\n",
      "  Best scaler: RobustScaler\n",
      "  Train: r = 0.9996 (p=2.666e-250), RMSE = 0.0169\n",
      "  Test: r = 0.0250 (p=0.8785), RMSE = 0.9785\n",
      "\n",
      "Summary of Results:\n",
      "      Layer Name      | Regression Model |   CV r   |  Test r  \n",
      "     block5_pool      |      Ridge       |0.3369|0.0250\n",
      "     block1_pool      |      Ridge       |0.0674|0.0184\n",
      "\n",
      "Comparing best results from all CNN architectures:\n",
      "  CNN Type  |      Best Layer      | Regression Model |  Test r  | p-value  \n",
      "   VGG16    |     block5_pool      |      Ridge       |0.0250|0.8785\n",
      "\n",
      "VGG16 performs best for Neuron 1\n",
      "Best layer: block5_pool\n",
      "Best regression model: Ridge\n",
      "Test Pearson's r: 0.0250 (p=0.8785)\n",
      "Runtime: 11.21 seconds (0.19 minutes)\n",
      "\n",
      "\n",
      "==========RESULT==========\n",
      "Best CNN: vgg16\n",
      "Best layer: block5_pool\n",
      "Best scaler: RobustScaler\n",
      "Best regression model: Ridge\n",
      "Best params: {'feature_selection__k': 'all', 'model__alpha': 10.0, 'scaler': RobustScaler()}\n",
      "Train Pearson's r: 0.9996\n",
      "Test Pearson's r: 0.0250\n"
     ]
    }
   ],
   "source": [
    "result = analyze_neuron(\n",
    "    neuron_idx=0,\n",
    "    train_iterator=train_iterator,\n",
    "    neural_response_train=neural_response_train,\n",
    "    cnn_models=['vgg16', 'resnet', 'efficientnet', 'inception'],\n",
    "    regression_models=['ridge', 'elasticnet', 'lasso', 'bayesianridge', 'randomforest', 'gbr', 'svr', 'linearsvr'],\n",
    "    max_layers=50\n",
    ")\n",
    "\n",
    "# Print quick summary\n",
    "print('\\n\\n'+'='*10+'RESULT'+'='*10)\n",
    "print(f\"Best CNN: {result['cnn_type']}\")\n",
    "print(f\"Best layer: {result['layer_name']}\")\n",
    "print(f\"Best scaler: {result['scaler_type']}\")\n",
    "print(f\"Best regression model: {result['regression_model_type']}\")\n",
    "print(f\"Best params: {result['best_params']}\")\n",
    "print(f\"Train Pearson's r: {result['train_pearson_r']:.4f}\")\n",
    "print(f\"Test Pearson's r: {result['test_pearson_r']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
